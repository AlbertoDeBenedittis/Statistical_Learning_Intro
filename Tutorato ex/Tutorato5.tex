% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Tutorato5},
  pdfauthor={Alberto De Benedittis},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Tutorato5}
\author{Alberto De Benedittis}
\date{9/4/2021}

\begin{document}
\maketitle

\hypertarget{exercise-1}{%
\section{Exercise 1}\label{exercise-1}}

The College dataset (contained in the ISLR library) collects statistics
measured on 18 variables for 777 US colleges. Some of the variables
include whether the college is a private or public institution, the
number of application received, the number of applications accepted,
etc. (for full details: ?College)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ISLR)}
\NormalTok{df <-}\StringTok{  }\NormalTok{ISLR}\OperatorTok{::}\NormalTok{College}
\end{Highlighting}
\end{Shaded}

Here, we want to predict the number of applications received using the
other variables.\\
\#\# Split the data into a training/test set.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{110}\NormalTok{)}
\NormalTok{train_in <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(df), }\FloatTok{.5}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(df))}
\NormalTok{train <-}\StringTok{  }\NormalTok{df[train_in,]}
\NormalTok{test <-}\StringTok{  }\NormalTok{df[}\OperatorTok{-}\NormalTok{train_in,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{fit-a-least-squares-linear-model-on-the-training-set-set-and-evaluate-the-error-on-the-test-set.}{%
\subsection{Fit a least squares linear model on the training set set and
evaluate the error on the test
set.}\label{fit-a-least-squares-linear-model-on-the-training-set-set-and-evaluate-the-error-on-the-test-set.}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm.fit <-}\StringTok{  }\KeywordTok{lm}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ df)}
\NormalTok{glm.sum <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(glm.fit)}
\end{Highlighting}
\end{Shaded}

According to the summary the variables that effect the choice of the
students for the University are: Private,Accept (this is quite expected
since it is clear that there is a correlation between the number of
accetted people and the people who send the
application),Enroll,Top10,Top25,Outstate,Roomboard,Phd (although it is a
borderline situation but as we know this could be cause by the seed) ,
Expend and grad.rate. Now that we have created the model on the training
set wwe want to test it on the validation set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm.pred <-}\StringTok{  }\KeywordTok{predict}\NormalTok{(glm.fit, test, }\DataTypeTok{type =} \StringTok{'response'}\NormalTok{)}
\NormalTok{MSE <-}\StringTok{  }\KeywordTok{mean}\NormalTok{((test}\OperatorTok{$}\NormalTok{Apps}\OperatorTok{-}\StringTok{ }\NormalTok{glm.pred )}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{fit-a-ridge-regression-model-on-the-training-set-choosing-ux3bb-by-cross-validation.-report-the-test-error.}{%
\subsection{Fit a ridge regression model on the training set, choosing Î»
by cross-validation. Report the test
error.}\label{fit-a-ridge-regression-model-on-the-training-set-choosing-ux3bb-by-cross-validation.-report-the-test-error.}}

\textbf{RIDGE}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'glmnet' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
## Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
## Loaded glmnet 4.1-1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{  }\KeywordTok{model.matrix}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{.,df)}
\NormalTok{y <-}\StringTok{  }\NormalTok{df}\OperatorTok{$}\NormalTok{Apps}
\NormalTok{grid <-}\StringTok{  }\DecValTok{10}\OperatorTok{^}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{-2}\NormalTok{, }\DataTypeTok{length =} \DecValTok{100}\NormalTok{) }
\CommentTok{#here we have chosen to implement the function over a grid of values ranging from lambda = 10^10 to lambda = 10^-2, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit.}
\NormalTok{ridge.fit <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x,y,}\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid)}
\CommentTok{# by default the glmnet standardizes the variables.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(}\KeywordTok{coef}\NormalTok{(ridge.fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  19 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Associated with each value of lambda is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a 19X100 matrix with 19 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of lambda)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred <-}\StringTok{  }\KeywordTok{predict}\NormalTok{(ridge.fit, }\DataTypeTok{s=}\DecValTok{4}\NormalTok{, }\DataTypeTok{newx =} \KeywordTok{model.matrix}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ test))}
\KeywordTok{mean}\NormalTok{((ridge.pred}\OperatorTok{-}\NormalTok{test}\OperatorTok{$}\NormalTok{Apps)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1085266
\end{verbatim}

In general, instead of arbitrary choosing lambda = 4, it would be better
to use cross-validation to choose the tuning parameter lambda. We can do
this using the built-in cross-validation function, \texttt{cv.glmnet()}.
By default, the function performs ten-fold cross-validation, though this
can be changed using the argument \texttt{nfolds}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.out <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha=}\DecValTok{0}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.out)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Tutorato5_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam <-}\StringTok{ }\NormalTok{cv.out}\OperatorTok{$}\NormalTok{lambda.min}
\KeywordTok{print}\NormalTok{(bestlam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 364.8993
\end{verbatim}

Therefore, we see that the value of lambda that results in the smallest
cross validation error is 316.11. Now, let's compute the test MSE
associated with this value of lambda

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge.pred_best <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridge.fit, }\DataTypeTok{s =}\NormalTok{ bestlam, }\DataTypeTok{newx =} \KeywordTok{model.matrix}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ test))}
\KeywordTok{mean}\NormalTok{((ridge.pred_best }\OperatorTok{-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{Apps)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1740199
\end{verbatim}

Finally, we refit our ridge regression model on the full data set, using
the value of lambda chosen by cross-validation, and examine the
coefficients estimates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{  }\KeywordTok{glmnet}\NormalTok{(}\KeywordTok{model.matrix}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{., df), df}\OperatorTok{$}\NormalTok{Apps, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{)}
\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{type =} \StringTok{'coefficients'}\NormalTok{, }\DataTypeTok{s =}\NormalTok{ bestlam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 19 x 1 sparse Matrix of class "dgCMatrix"
##                         1
## (Intercept) -1.468326e+03
## (Intercept)  .           
## PrivateYes  -5.278781e+02
## Accept       1.004588e+00
## Enroll       4.313442e-01
## Top10perc    2.580619e+01
## Top25perc    5.501092e-01
## F.Undergrad  7.258520e-02
## P.Undergrad  2.420595e-02
## Outstate    -2.407454e-02
## Room.Board   1.987732e-01
## Books        1.285477e-01
## Personal    -8.146131e-03
## PhD         -4.028284e+00
## Terminal    -4.811071e+00
## S.F.Ratio    1.302180e+01
## perc.alumni -8.544783e+00
## Expend       7.589013e-02
## Grad.Rate    1.126699e+01
\end{verbatim}

As expected, none of the coefficients are zero-ridge regression does not
perform variable selection. \textbf{LASSO}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso.fit <-}\StringTok{  }\KeywordTok{glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid)}
\KeywordTok{plot}\NormalTok{(lasso.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):
## collapsing to unique 'x' values
\end{verbatim}

\includegraphics{Tutorato5_files/figure-latex/unnamed-chunk-11-1.pdf} We
can see from the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients will be exactly equal to 0. We now
perform cross validation and compute the associated test error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.out2 <-}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(x, y, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cv.out2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Tutorato5_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bestlam2 <-}\StringTok{  }\NormalTok{cv.out2}\OperatorTok{$}\NormalTok{lambda.min}
\NormalTok{lasso.pred <-}\StringTok{  }\KeywordTok{predict}\NormalTok{(lasso.fit, }\DataTypeTok{s =}\NormalTok{ bestlam2, }\DataTypeTok{newx =} \KeywordTok{model.matrix}\NormalTok{(Apps}\OperatorTok{~}\NormalTok{., test))}
\KeywordTok{mean}\NormalTok{((lasso.pred }\OperatorTok{-}\StringTok{ }\NormalTok{test}\OperatorTok{$}\NormalTok{Apps)}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1091147
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out2 <-}\StringTok{  }\KeywordTok{glmnet}\NormalTok{(x,y,}\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ grid)}
\NormalTok{lasso.coef <-}\StringTok{  }\KeywordTok{predict}\NormalTok{(out, }\DataTypeTok{type =} \StringTok{'coefficients'}\NormalTok{, }\DataTypeTok{s =}\NormalTok{ bestlam2)}
\NormalTok{lasso.coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 19 x 1 sparse Matrix of class "dgCMatrix"
##                         1
## (Intercept) -1.468326e+03
## (Intercept)  .           
## PrivateYes  -5.278781e+02
## Accept       1.004588e+00
## Enroll       4.313442e-01
## Top10perc    2.580619e+01
## Top25perc    5.501092e-01
## F.Undergrad  7.258520e-02
## P.Undergrad  2.420595e-02
## Outstate    -2.407454e-02
## Room.Board   1.987732e-01
## Books        1.285477e-01
## Personal    -8.146131e-03
## PhD         -4.028284e+00
## Terminal    -4.811071e+00
## S.F.Ratio    1.302180e+01
## perc.alumni -8.544783e+00
## Expend       7.589013e-02
## Grad.Rate    1.126699e+01
\end{verbatim}

The lasso has a substantial advantage over ridge regression in that the
resulting coefficients estimates are sparse.

\hypertarget{exercise-2}{%
\section{Exercise 2}\label{exercise-2}}

The Boston data (MASS library) contains housing values in the suburbs of
Boston for a sample of 506 observations. The aim is to predict per
capita crime rate (crim) from the other variables. a. Try out some of
the regression methods you learned so far, such as subset selection
(forward and backward), lasso, ridge regression. Present and discuss
results for the approaches that you consider. b. Propose a model (or set
of models) that seem to perform well on this data set, and justify your
answer. Make sure that you are evaluating model performance using
validation set error, cross-validation, or 1 some other reasonable
alternative, as opposed to using training error. c.~Inspect your
selected model. Does it involve all of the features in the data set?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\KeywordTok{library}\NormalTok{(leaps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'leaps' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boston <-}\StringTok{  }\NormalTok{MASS}\OperatorTok{::}\NormalTok{Boston}
\NormalTok{regfit.full <-}\StringTok{  }\KeywordTok{regsubsets}\NormalTok{(crim}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data =}\NormalTok{ boston, }\DataTypeTok{nvmax =} \DecValTok{14}\NormalTok{)}
\NormalTok{reg.summary <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(regfit.full)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{# RSS}
\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{rss, }\DataTypeTok{xlab =} \StringTok{'Number of variables'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'RSS'}\NormalTok{, }\DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'RSS'}\NormalTok{)}
\CommentTok{# ADJ R^2}
\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{adjr2, }\DataTypeTok{xlab =} \StringTok{'Number of variables'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Adjusted RSq'}\NormalTok{, }\DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'Adjusted R-squared'}\NormalTok{)}
\NormalTok{Rsq_p <-}\StringTok{  }\KeywordTok{which.max}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{adjr2)}
\KeywordTok{points}\NormalTok{(Rsq_p, reg.summary}\OperatorTok{$}\NormalTok{adjr2[Rsq_p], }\DataTypeTok{col =} \StringTok{'indianred'}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\CommentTok{# CP MALLOW}
\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{cp, }\DataTypeTok{xlab =} \StringTok{'Number of variables'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'Cp}\CharTok{\textbackslash{}'}\StringTok{Mallow'}\NormalTok{, }\DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'CP}\CharTok{\textbackslash{}'}\StringTok{s Mallow'}\NormalTok{)}
\NormalTok{CP_p <-}\StringTok{  }\KeywordTok{which.min}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{cp)}
  \KeywordTok{points}\NormalTok{(CP_p, reg.summary}\OperatorTok{$}\NormalTok{cp[CP_p], }\DataTypeTok{col =} \StringTok{'indianred'}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\CommentTok{# BIC}
\KeywordTok{plot}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{bic, }\DataTypeTok{xlab =} \StringTok{'Number of variables'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'BIC'}\NormalTok{, }\DataTypeTok{type =} \StringTok{'l'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'BIC'}\NormalTok{)}
\NormalTok{bic_p <-}\StringTok{  }\KeywordTok{which.min}\NormalTok{(reg.summary}\OperatorTok{$}\NormalTok{bic)}
\KeywordTok{points}\NormalTok{(bic_p, reg.summary}\OperatorTok{$}\NormalTok{bic[bic_p], }\DataTypeTok{col =} \StringTok{'indianred'}\NormalTok{, }\DataTypeTok{cex =} \DecValTok{2}\NormalTok{, }\DataTypeTok{pch =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Tutorato5_files/figure-latex/unnamed-chunk-14-1.pdf}
Wow, here it is difficult to assess which is the best model because the
Adjusted R-squared, CP's Mallow and the BIC suggest quite different
options. Indeed, the CP's Mallow and the Adjusted R-squared suggest that
the best model is the one with 8 variable (Actually, the adjusted
R-squared suggest the model with 9 variables, but since there is not
much difference and the situation is already quite complicated, we
prefer to say that these two methods suggest the same result. Moreover,
according to the one standard error rule when we are uncertain between
two models we should always prefer the simplest one if there is not a
huge difference). On the other hand, the BIC shows that the model with
three variable is the best one.

\end{document}
